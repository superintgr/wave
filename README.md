# wave
Constructors whose object systems cause interference patterns.

**Standard interference workflow**

- An object instantiates code vectors whose object system consists of multidimensional subsystem parts,
- Any differentiable change brought upon into the objects static constitutions, would correspond to state vectors whose identity information does not point to single unique source,
- An unique source or target constitute a sharp observable set of quantities and the property is universal across all of the relative states within the subsystems code,
- Differentiable actions since conservative of invarient and symmetrical code or properties, will remain true although with branching complex introduced into the mix whenever interference is made impossible,
- Universal properties being true and their laws being universally effective, all effected branches that does not converge onto the same identity variables, either change irreversibly or sparsely remain connected to neighboring universes,
- If irreversible changes in identity does not meaningfully describe any of the descriptable attributes of the known universes, are considered unlike any of the source universes as they meaningfully violate the attributable criteria for information to be possible.


> The above do indeed outline the workflow or could be interpreted otherwise, as there are no operational description to specific interferring property for which the construction could be overseen.

## Structure of interferring waves:

- substrate specific property
- disjoint attribute
- physical variable
- logical variable
- abstract information
- implicit definition
- generic constructor
- regular network
- locally inaccessible information
- angular momentum

## Substrate Specific Property

> ref: [strata, datum]

Consider a stack representing the geological stratum of an universe. The physics dictates that there be conservation of energy in each of the possible strata within the composition of tbe structure that encodes all the tensorial information.

The tensor that parameterizes that stack must be of one so that their model instances will always preserve the intrinsic symmetry in their parametrization scheme such that whenever an instance causes a differentiable change which results in now multiple other instances, and each of them are jointly isolated, their combined effect conserves the same net quantity with which the original instance had retained fixed point outputs.

For example the `GPT2` checkpoints constitute a closed system and as long as the layers are frozen thier transformations will always cause the same patterns in final state.

## Disjoint Attributes

If we prepare a new substrate with its base layer set to use the `GPT2` checkpoints, the frozen layers will represent the `disjoint attributes`, where the causal outputs from the base layer constitutes the **ground state** of the new substrate.

The physical variables are those quantitites whose substrates have the normalized outputs from `base layer` and their **logical state** attributes `x` in set `X` have all `x * x ~ x'` and `abs(x') == 1`.

That is for all differentiable chages caused by these frozen attributes will contribute a sharp state regardless of the measurable quantity.

**Notes**
- `measurable quantity`(s) are those abstract variables whose causal states are possible and does not require anything but the constructor which causes some dependent subsystem to remain possible (*attached*)

## Physical Variable

> ref: [downstream oxygen sensor, autoencoders, copies minimum to parameters without learning]

***Floating Natives***
>> **autoencoders** learn features with unreasonable proportion of hidden parameters in their learnable layers. this leads to faster drops in loss trends during training however testing reveals very unreasonable learning behavior too.
>> the explanation requires the dynamical changes made to prepare the trend which exposes the network to regular training data and the patterns of activities for the whole period in picture.
>> when the training steps are inspected carefully, it typically consists of the following `static` pattern:
>> ... input, output = train_set
>> ... prediction = model(input)
>> ... loss = criterion(prediction, output)
>> where the loss produces a landscape encoding all the information related to what the model responds with and how its response corresponds with the impulse caused the model to respond. however the impulse is not the input directly, and rather how the causal attributes constructed from exposing one of the states of the expected system given to the model as input justifies one of the associated other state of that same type of system as resembling the output expected.
>> **notice** that the expectation values and dynamical interactions are both follow from the usual group of parameter families [`static`, `dynamic`, `state`] forming the constitutional attributes of any system; *however* the evolution of each of these two systems do not follow any `observable path` on which neither of the two systems are possible to have at least one of the `medium attribute` for `information state` to be possible since no medium is asserted to have been present from the fundamental principle behind training such a `random-like` substrate.
>> **notice** the fundamental principle asserts some regularity in nature that computers do conform to obey in their processing of information variables, hence the word `principle'. with that notion nothing like *random* attribute could ever be caused to represnt *deterministic* attribute, especially when the substrate is asserted to be in random initialized state. therefore ***no such principle exists for a network that initialize non deterministic states*** and ***for any regularity to be measured, there must be a regularity in instantiations in states for all possible states where at least two of the states are deterministic*** proves that `no random network could produce deterministic observable` for which `all non perturbing measurement`(s) encoding `stochastic variables in state` satisfies the definition of `regular network of nodes' while preserving `linear superposition of states` to be possible (i.e. objective regularities in nature and principles of physical laws)
>> **acknowledge** that the very possibility of any record to consists such a statistical information requires that there exists absolutely **deterministic** instantes of states because something remains in existence invarient to any observable that is sharp in any physical network there exists.
>> **finally** because there cannot always be a sharp measurable quantity in any systems at all, that intrinsic variable is designed out of training algorithms due to bad theories explaning computation and information.
