# wave
Constructors whose object systems cause interference patterns.

**Standard interference workflow**

- An object instantiates code vectors whose object system consists of multidimensional subsystem parts,
- Any differentiable change brought upon into the objects static constitutions, would correspond to state vectors whose identity information does not point to single unique source,
- An unique source or target constitute a sharp observable set of quantities and the property is universal across all of the relative states within the subsystems code,
- Differentiable actions since conservative of invarient and symmetrical code or properties, will remain true although with branching complex introduced into the mix whenever interference is made impossible,
- Universal properties being true and their laws being universally effective, all effected branches that does not converge onto the same identity variables, either change irreversibly or sparsely remain connected to neighboring universes,
- If irreversible changes in identity does not meaningfully describe any of the descriptable attributes of the known universes, are considered unlike any of the source universes as they meaningfully violate the attributable criteria for information to be possible.


> The above do indeed outline the workflow or could be interpreted otherwise, as there are no operational description to specific interferring property for which the construction could be overseen.

## Structure of interferring waves:

- substrate specific property
- disjoint attribute
- physical variable
- logical variable
- abstract information
- implicit definition
- generic constructor
- regular network
- locally inaccessible information
- angular momentum

## Substrate Specific Property

> ref: [strata, datum]

Consider a stack representing the geological stratum of an universe. The physics dictates that there be conservation of energy in each of the possible strata within the composition of tbe structure that encodes all the tensorial information.

The tensor that parameterizes that stack must be of one so that their model instances will always preserve the intrinsic symmetry in their parametrization scheme such that whenever an instance causes a differentiable change which results in now multiple other instances, and each of them are jointly isolated, their combined effect conserves the same net quantity with which the original instance had retained fixed point outputs.

For example the `GPT2` checkpoints constitute a closed system and as long as the layers are frozen thier transformations will always cause the same patterns in final state.

## Disjoint Attributes

If we prepare a new substrate with its base layer set to use the `GPT2` checkpoints, the frozen layers will represent the `disjoint attributes`, where the causal outputs from the base layer constitutes the **ground state** of the new substrate.

The physical variables are those quantitites whose substrates have the normalized outputs from `base layer` and their **logical state** attributes `x` in set `X` have all `x * x ~ x'` and `abs(x') == 1`.

That is for all differentiable chages caused by these frozen attributes will contribute a sharp state regardless of the measurable quantity.

**Notes**
- `measurable quantity`(s) are those abstract variables whose causal states are possible and does not require anything but the constructor which causes some dependent subsystem to remain possible (*attached*)

## Physical Variable

> ref: [downstream oxygen sensor, autoencoders, copies minimum to parameters without learning]

***Floating Natives***
>> **autoencoders** learn features with unreasonable proportion of hidden parameters in their learnable layers. this leads to faster drops in loss trends during training however testing reveals very unreasonable learning behavior too.
>> the explanation requires the dynamical changes made to prepare the trend which exposes the network to regular training data and the patterns of activities for the whole period in picture.
>> when the training steps are inspected carefully, it typically consists of the following `static` pattern:
>> ... input, output = train_set
>> ... prediction = model(input)
>> ... loss = criterion(prediction, output)
>> where the loss produces a landscape encoding all the information related to what the model responds with and how its response corresponds with the impulse caused the model to respond. however the impulse is not the input directly, and rather how the causal attributes constructed from exposing one of the states of the expected system given to the model as input justifies one of the associated other state of that same type of system as resembling the output expected.
>> **notice** that the expectation values and dynamical interactions are both follow from the usual group of parameter families [`static`, `dynamic`, `state`] forming the constitutional attributes of any system; *however* the evolution of each of these two systems do not follow any `observable path` on which neither of the two systems are possible to have at least one of the `medium attribute` for `information state` to be possible since no medium is asserted to have been present from the fundamental principle behind training such a `random-like` substrate.
>> **notice** the fundamental principle asserts some regularity in nature that computers do conform to obey in their processing of information variables, hence the word `principle'. with that notion nothing like *random* attribute could ever be caused to represnt *deterministic* attribute, especially when the substrate is asserted to be in random initialized state. therefore ***no such principle exists for a network that initialize non deterministic states*** and ***for any regularity to be measured, there must be a regularity in instantiations in states for all possible states where at least two of the states are deterministic*** proves that `no random network could produce deterministic observable` for which `all non perturbing measurement`(s) encoding `stochastic variables in state` satisfies the definition of `regular network of nodes' while preserving `linear superposition of states` to be possible (i.e. objective regularities in nature and principles of physical laws)
>> **acknowledge** that the very possibility of any record to consists such a statistical information requires that there exists absolutely **deterministic** instantes of states because something remains in existence invarient to any observable that is sharp in any physical network there exists.
>> **finally** because there cannot always be a sharp measurable quantity in any systems at all, that intrinsic variable is designed out of training algorithms due to bad theories explaning computation and information.

We introduce the simulation of a downstream oxygen sensor below:

Oxidation states of carbon atoms contain the set `<-4, -3, -2, -1, 0, +1, +2, +3, +4>` [see `thought vector`, `understanding gradient` or `undergrad`, `state vector`]
**The oxization state here describes for `standard tempetature and pressure` which encodes the `solid` attributes into the `phase` substrate for any property of possible type.

It has some `triple point` representation encoding the states for which all of the possible `phase constructor` exists in thermodynamic equilibrium which we call `super information` substrate and the constructors as `super integer` computatation medium. [see `tensor`, `tensile strength`, `native profile`]

**Recurrent Processes**

Typically recurrent networks of neurons work in the following style:
- at every step, it expects an input signal and an additional memory information consituting last hidden state or some convolved transition of internal configuration associated with how the network produces new state
- the result of a feedforward transform is a pair: `output state`, `state variables`
- chaining further any effect related to the generated states for subsequent feedforward events

The output state it produces only depend on the hidden state it returns along side, assuming no drastic non-linear changes made to the parameters.

**non linear** is meant here as physical evolution of the density constructor that does not evolve in unitary motion.

The `unitary` refers to unit correspondence, which states that all linear changes in logically related functions of the participating variables move with finite means in computation process.

**Computations**

1. An initialized model is made to produce the right visible state either in output production or intermediate levels
2. There is a set of features and their mapping to code which executes the feature image. Such a record usually contains possible set of features as key and associated criteria for realizing any such having permutations applied to it and code objects to manage the states of those features,

For example:

- `excaldraw` -> `dictionary` at step `t + 1`
- at step `t - 1`, the dictionary must have had the same keys
- the set of invarient `key` instances are called features
- `feature set` maps any change between two states `(state[t + 1], state[t - 1])` using a *vector* of associated `variable` as parameters.
- the model that learns to identify the set of features are parametric networks with coefficients appying `transformation`(s) to the variables

**Linear Transformation**

Changes made between two states to a variable records information necessary to handle the change.

**The model must identify all changes not limited to higher level transitions**.

We setup a server of states with fixed duration and uniform interval between transitions.
- at each timestep, the server either has detected changes or making changes to existing files
- for any change detected, it encodes the data contents into a vector of features
- from the data features, the vector is transformed by one or more models specific to the feature components in it
- the transformation is decoded back to another feature vector where the new state must be translated into data values
- decoded data is postprocessed into the format required and sent to the server for confirming changes

**Interpreter and Callback**

In the design window, I shall build structures that may require executing some code in order to derive intermediate state within the window.

However, I am going to design or plan in terms of code or programming objects to cause equivalent possible transformation but rather my workflow will only rise to the coding level, if and only if some variable is required to have computation performed because it would not otherwise be optimal.

- `static` variables are the one that do not require change to be computed and `dynamic` states require those variables to participate.
- in my work model, the entire collection of elements are bi-stably static.
- 'bi-stable'-ity means that high level design -> context dependent change, context dependent change -> high level design -- while all change is finite and made within a bounded area of elastic states

For example,

In the design environment, the following graph is embedded:
    ```
    block name : displaying ascii characters with integer tokens
    stack : [python interpreter -> block code -> dictionary mapping -> block code -> python interpreter]

    block name : fetching the printed information from last block
    stack : [python interpreter -> block code -> snapshot -> dictionary mapping -> query -> key -> value -> python interpreter]

    block name : putting all changes together
    ceiling : ...
    levels : ...
    floor : ...
    ```
...
THIS BRANCH IS BEING ARCHIVED. SEE ENVIRONMENT PATH.
